{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt, rdflib, pandas as pd, numpy as np, sys, os, random, math, fiona, uuid, copy, glob\n",
    "from osgeo import gdal, osr, gdal_array\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from typing import *\n",
    "from ruamel.yaml import YAML\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# next cell\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drepr version: 2.9\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(verbose=True)\n",
    "paths = [\"../\", \"/workspace/d-repr/pydrepr\", \"/home/rook/workspace/d-repr/pydrepr\"]\n",
    "for path in paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "from drepr import __version__, DRepr, outputs\n",
    "from drepr.executors.readers.reader_container import ReaderContainer\n",
    "from drepr.executors.readers.np_dict import NPDictReader\n",
    "print(\"drepr version:\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DcatReadFunc\n",
    "from funcs.trans_cropping_func import CroppingTransFunc\n",
    "from funcs.readers.dcat_read_func import ShardedClassID, ShardedBackend\n",
    "from funcs.gdal.raster import *\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. configuration & global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ['HOME_DIR']\n",
    "\n",
    "gldas = \"5babae3f-c468-4e01-862e-8b201468e3b5\"\n",
    "gpm = \"ea0e86f3-9470-4e7e-a581-df85b4a7075d\"\n",
    "region = \"74e6f707-d5e9-4cbd-ae26-16ffa21a1d84\"\n",
    "variable = \"atmosphere_water__precipitation_mass_flux\"\n",
    "variable = \"land_surface_air__temperature\"\n",
    "\n",
    "ethiopia = BoundingBox(32.75418, 3.22206, 47.98942, 15.15943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. download the weather dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_datasets(dataset_id, start_time, end_time):\n",
    "  if start_time is not None:\n",
    "    start_time = parse(start_time)\n",
    "  if end_time is not None:\n",
    "    end_time = parse(end_time)\n",
    "    \n",
    "  func = DcatReadFunc(dataset_id, start_time, end_time)\n",
    "  func.set_preferences({\"data\": \"array\"})\n",
    "  datasets = func.exec()['data']\n",
    "  return datasets\n",
    "\n",
    "def read_local_datasets(repr_file, resource_path):\n",
    "  drepr = DRepr.parse_from_file(repr_file)\n",
    "  files = glob.glob(resource_path)\n",
    "  \n",
    "  if len(files) == 1:\n",
    "    return outputs.ArrayBackend.from_drepr(drepr, file)[0]\n",
    "  \n",
    "  ds = ShardedBackend(len(files))\n",
    "  for file in files:\n",
    "    ds.add(outputs.ArrayBackend.from_drepr(drepr, file, ds.inject_class_id))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-04 21:07:09,880 | funcs.readers.dcat_read_func | INFO - Overwrite GLDAS\n",
      "2020-03-04 21:07:09,880 | funcs.readers.dcat_read_func | INFO - Found key 'resource_repr'\n",
      "2020-03-04 21:07:09,881 | funcs.readers.dcat_read_func | INFO - Downloading 32 resources ...\n",
      "2020-03-04 21:07:09,882 | funcs.readers.dcat_read_func | INFO - Download Complete. Skip 32 and download 0 resources\n"
     ]
    }
   ],
   "source": [
    "weather_dataset = read_datasets(gldas, \"2011-09-01T00:00:00\", \"2011-09-05T00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. crop the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful functions to convert datasets to rasters and convert them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + \"/examples/d3m/crop_bb.yml\", \"r\") as f:\n",
    "  crop_bb_conf = yaml.load(f)\n",
    "\n",
    "def dataset2raster(sm, variable):\n",
    "  rasters = []\n",
    "  for c in sm.c('mint:Variable').filter(outputs.FCondition(\"mint:standardName\", \"==\", variable)):\n",
    "    for raster_id, sc in c.group_by(\"mint-geo:raster\"):\n",
    "      # TODO: handle time properly\n",
    "      timestamp = sc.p(\"mint:timestamp\").as_ndarray([])\n",
    "      if timestamp.data.size != 1:\n",
    "        raise NotImplemented()\n",
    "      timestamp = timestamp.data[0]\n",
    "      \n",
    "      data = sc.p(\"rdf:value\").as_ndarray([sc.p(\"mint-geo:lat\"), sc.p(\"mint-geo:long\")])\n",
    "      gt_info = sm.get_record_by_id(raster_id)\n",
    "      gt = GeoTransform(x_0=gt_info.s(\"mint-geo:x_0\"),\n",
    "                        y_0=gt_info.s(\"mint-geo:y_0\"),\n",
    "                        dx=gt_info.s(\"mint-geo:dx\"), dy=gt_info.s(\"mint-geo:dy\"))\n",
    "      raster = Raster(data.data, gt, int(gt_info.s(\"mint-geo:epsg\")),\n",
    "             float(data.nodata.value) if data.nodata is not None else None)\n",
    "      raster.timestamp = timestamp\n",
    "      rasters.append(raster)\n",
    "  return rasters\n",
    "  \n",
    "def raster2dataset(r, variable):\n",
    "  global crop_bb_conf\n",
    "  reader = NPDictReader({\n",
    "    \"variable\": r.data,\n",
    "    \"lat\": r.get_center_latitude(),\n",
    "    \"long\": r.get_center_longitude(),\n",
    "    \"timestamp\": r.timestamp,\n",
    "    \"standard_name\": variable,\n",
    "    \"gt_x_0\": r.geotransform.x_0,\n",
    "    \"gt_y_0\": r.geotransform.y_0,\n",
    "    \"gt_dx\": r.geotransform.dx,\n",
    "    \"gt_dy\": r.geotransform.dy,\n",
    "    \"gt_epsg\": r.epsg,\n",
    "    \"gt_x_slope\": r.geotransform.x_slope,\n",
    "    \"gt_y_slope\": r.geotransform.y_slope,\n",
    "  })\n",
    "  resource_id = str(uuid.uuid4())\n",
    "  ReaderContainer.get_instance().set(resource_id, reader)\n",
    "  \n",
    "  conf = copy.deepcopy(crop_bb_conf)\n",
    "  conf['attributes']['variable']['missing_values'].append(r.nodata)\n",
    "  drepr = DRepr.parse(conf)\n",
    "  sm = outputs.ArrayBackend.from_drepr(drepr, resource_id)\n",
    "  ReaderContainer.get_instance().delete(resource_id)\n",
    "  return sm\n",
    "\n",
    "def raster2netcdf(r, variable, outfile):\n",
    "  lat = r.get_center_latitude()\n",
    "  long = r.get_center_longitude()\n",
    "  data = xr.DataArray(r.data, dims=('lat', 'long'), coords={'lat': lat, 'long': long})\n",
    "  data.attrs['standard_name'] = variable\n",
    "  data.attrs['_FillValue'] = r.nodata\n",
    "  data.attrs['missing_values'] = r.nodata\n",
    "  \n",
    "  ds = xr.Dataset({standard_name: data})  \n",
    "  ds.to_netcdf(outfile)\n",
    "  \n",
    "def dataset2netcdf(sm):\n",
    "  assert len(sm.c(\"mint:Variable\")) == 1\n",
    "  c = sm.c(\"mint:Variable\")[0]\n",
    "  if c.p(\"mint:Place\") is not None:\n",
    "    raise NotImplemented()\n",
    "    \n",
    "  standard_name = c.p(\"mint:standardName\").as_ndarray([]).data\n",
    "  assert standard_name.size == 1\n",
    "  standard_name = standard_name[0]\n",
    "\n",
    "  timestamp = c.p(\"mint:timestamp\").as_ndarray([]).data\n",
    "  assert timestamp.size == 1\n",
    "  timestamp = timestamp[0]\n",
    "  \n",
    "  groups = list(c.group_by(\"mint-geo:raster\"))\n",
    "  assert len(groups) == 1\n",
    "  gt = sm.get_record_by_id(groups[0][0])\n",
    "\n",
    "  val = c.p(\"rdf:value\").as_ndarray([c.p(\"mint-geo:lat\"), c.p(\"mint-geo:long\")])\n",
    "  data = val.data.reshape(1, *val.data.shape)\n",
    "  data = xr.DataArray(val.data.reshape(1, *val.data.shape), dims=('time', 'lat', 'long'), coords={\n",
    "    'lat': val.index_props[0], 'long': val.index_props[1], 'time': np.asarray([timestamp])\n",
    "  })\n",
    "  data.attrs['standard_name'] = standard_name\n",
    "  data.attrs['_FillValue'] = val.nodata.value\n",
    "  data.attrs['missing_values'] = val.nodata.value\n",
    "  \n",
    "  ds = xr.Dataset({\"variable\": data})\n",
    "  ds.attrs.update({\n",
    "    \"conventions\": \"CF-1.6\",\n",
    "    \"dx\": gt.s('mint-geo:dx'),\n",
    "    \"dy\": gt.s(\"mint-geo:dy\"),\n",
    "    \"epsg\": gt.s(\"mint-geo:epsg\"),\n",
    "    \"x_slope\": gt.s(\"mint-geo:x_slope\"),\n",
    "    \"y_slope\": gt.s(\"mint-geo:y_slope\"),\n",
    "    \"x_0\": gt.s(\"mint-geo:x_0\"),\n",
    "    \"y_0\": gt.s(\"mint-geo:y_0\")\n",
    "  })\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 crop the data by a bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2506d666543443b8020853ab5adafab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subrasters = []\n",
    "for raster in tqdm(dataset2raster(weather_dataset, variable)):\n",
    "  sr = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  sr.timestamp = raster.timestamp\n",
    "  filename = datetime.datetime.utcfromtimestamp(sr.timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "  sm = raster2dataset(sr, variable)\n",
    "  dataset2netcdf(sm).to_netcdf(HOME_DIR + f\"/data/gldas/{variable}/{filename}.nc4\")\n",
    "  subrasters.append(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug to see if the data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters[0].to_geotiff(HOME_DIR + \"/data/debug/small.tif\")\n",
    "raster.to_geotiff(HOME_DIR + \"/data/debug/full.tif\")  # incorrect because \n",
    "sm = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", \n",
    "                         HOME_DIR + f\"/data/gldas/{variable}/20110901000000.nc4\")\n",
    "a = dataset2raster(sm, variable)[0].data\n",
    "b = subrasters[0].data\n",
    "\n",
    "assert np.allclose(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 crop data by shapefiles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_array_to_shapefile(data, fname):\n",
    "  polygon = data[0]\n",
    "  if isinstance(polygon[0][0][0], (int, float)):\n",
    "    shape_type = 'Polygon'\n",
    "  else:\n",
    "    shape_type = 'MultiPolygon'\n",
    "\n",
    "  epsg = fiona.crs.from_epsg(data[1])\n",
    "  driver = \"ESRI Shapefile\"\n",
    "  polygon = {\n",
    "      'geometry': {\n",
    "          'type': shape_type,\n",
    "          'coordinates': polygon\n",
    "      },\n",
    "      'properties': {\n",
    "          'name': 'TempCroppingPolygon'\n",
    "      }\n",
    "  }\n",
    "  schema = {'geometry': shape_type, 'properties': {'name': 'str'}}\n",
    "  with fiona.open(fname, 'w', crs='+datum=WGS84 +ellps=WGS84 +no_defs +proj=longlat', driver=driver, schema=schema) as shapefile:\n",
    "    shapefile.write(polygon)\n",
    "    \n",
    "def create_shapefile(sm, dname):\n",
    "  shape_files = []\n",
    "  for c in sm.c(\"mint:Place\"):\n",
    "    for r in c.iter_records():\n",
    "      polygon = sm.get_record_by_id(r.s('mint-geo:bounding')).s('rdf:value')\n",
    "      shape_file = HOME_DIR + f'/data/{dname}/{r.s(\"mint:region\").replace(\" \", \"-\")}.shp'\n",
    "      shape_array_to_shapefile([polygon, 4326], shape_file)\n",
    "      shape_files.append({\n",
    "        \"file\": shape_file,\n",
    "        \"region\": r.s(\"mint:region\")\n",
    "      })\n",
    "  return shape_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-04 21:08:51,925 | funcs.readers.dcat_read_func | INFO - Found key 'dataset_repr'\n",
      "2020-03-04 21:08:51,925 | funcs.readers.dcat_read_func | INFO - Downloading 1 resources ...\n",
      "2020-03-04 21:08:51,927 | funcs.readers.dcat_read_func | INFO - Download Complete. Skip 1 and download 0 resources\n"
     ]
    }
   ],
   "source": [
    "region_dataset = read_datasets(region, None, None)\n",
    "shape_files = create_shapefile(region_dataset, 'debug/regions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select datasets to crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = \"201109*.nc4\"\n",
    "temp_dataset = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", \n",
    "                                   HOME_DIR + f\"/data/gldas/{variable}/{date_pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0647f7dfc31404b965799c58b853894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data = []\n",
    "\n",
    "for raster in tqdm(dataset2raster(temp_dataset, variable)):\n",
    "  for shape_file in shape_files:\n",
    "    sr = raster.crop(vector_file=shape_file['file'],\n",
    "                      resampling_algo=ReSample.BILINEAR,\n",
    "                      touch_cutline=True)\n",
    "    temp_data.append({\n",
    "      \"data\": sr.data,\n",
    "      \"nodata\": sr.nodata,\n",
    "      \"region\": shape_file['region'],\n",
    "      \"timestamp\": raster.timestamp,\n",
    "      \"timestring\": datetime.datetime.utcfromtimestamp(raster.timestamp).strftime(\"%Y%m%d%H%M%S\"),\n",
    "      \"date\": datetime.datetime.utcfromtimestamp(raster.timestamp).strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "#     sr.to_geotiff(HOME_DIR + f\"/data/region_{shape_file['region'].replace(' ', '-')}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. calculate average daily temperature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21ac13af5784e658b00f29878e9590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=352.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "groups = defaultdict(list)\n",
    "for d in tqdm(temp_data):\n",
    "  groups[(d['date'], d['region'])].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "i = 0\n",
    "for (date, region), meshs in groups.items():\n",
    "  i += 1\n",
    "#   print(i)\n",
    "#   if i != 1:\n",
    "#     continue\n",
    "  \n",
    "  n_obs_mesh = np.zeros(meshs[0]['data'].shape, dtype=np.int64)\n",
    "  sum_mesh = np.zeros(meshs[0]['data'].shape, dtype=np.float64)\n",
    "  for mesh in meshs:\n",
    "    d = mesh['data']\n",
    "    nodata = mesh['nodata']\n",
    "    \n",
    "    mask = (d != nodata).astype(np.int64)\n",
    "    # count the reported points and their values\n",
    "    n_obs_mesh += mask\n",
    "    sum_mesh += mask * d\n",
    "  \n",
    "  obs_mask = n_obs_mesh != 0\n",
    "  avg_daily_temp_mesh = sum_mesh[obs_mask] / n_obs_mesh[obs_mask]\n",
    "  avg_daily_temp = np.average(avg_daily_temp_mesh) - 273.15\n",
    "  \n",
    "  rows.append({\"date\": date, \"region\": region, \"avg_daily_temp (celsius)\": avg_daily_temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>avg_daily_temp (celsius)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>14.236616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Afar</td>\n",
       "      <td>31.898066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Amhara</td>\n",
       "      <td>18.525962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Beneshangul Gumu</td>\n",
       "      <td>20.164072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Dire Dawa</td>\n",
       "      <td>23.763960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Gambela</td>\n",
       "      <td>22.438086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Hareri</td>\n",
       "      <td>20.111247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>19.211197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>SNNPR</td>\n",
       "      <td>19.245293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>Somali</td>\n",
       "      <td>27.533514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date            region  avg_daily_temp (celsius)\n",
       "11  2011-09-01       Addis Ababa                 14.236616\n",
       "20  2011-09-01              Afar                 31.898066\n",
       "19  2011-09-01            Amhara                 18.525962\n",
       "18  2011-09-01  Beneshangul Gumu                 20.164072\n",
       "14  2011-09-01         Dire Dawa                 23.763960\n",
       "17  2011-09-01           Gambela                 22.438086\n",
       "15  2011-09-01            Hareri                 20.111247\n",
       "21  2011-09-01            Oromia                 19.211197\n",
       "16  2011-09-01             SNNPR                 19.245293\n",
       "13  2011-09-01            Somali                 27.533514"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df = df.sort_values(['date', 'region'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(HOME_DIR + \"/data/avg_daily_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintdt",
   "language": "python",
   "name": "mintdt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
