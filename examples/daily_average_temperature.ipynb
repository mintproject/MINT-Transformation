{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, rdflib, pandas as pd, numpy as np, sys, os, random, math, fiona, uuid, copy, glob, ujson\n",
    "from osgeo import gdal, osr, gdal_array\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from typing import *\n",
    "from ruamel.yaml import YAML\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# next cell\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(verbose=True)\n",
    "paths = [\"../\", \"/workspace/d-repr/pydrepr\", \"/home/rook/workspace/d-repr/pydrepr\"]\n",
    "for path in paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "from drepr import __version__, DRepr, outputs\n",
    "from drepr.executors.readers.reader_container import ReaderContainer\n",
    "from drepr.executors.readers.np_dict import NPDictReader\n",
    "print(\"drepr version:\", __version__)\n",
    "import os\n",
    "os.environ['DCAT_URL'] = 'https://api.mint-data-catalog.org'\n",
    "os.environ['DATA_CATALOG_DOWNLOAD_DIR'] = '../tmp/dcat_read_func'\n",
    "os.environ['HOME_DIR'] = '../'\n",
    "os.environ['NO_CHECK_CERTIFICATE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from funcs import DcatReadFunc\n",
    "from funcs import CroppingTransFunc\n",
    "from dtran.backend import ShardedClassID, ShardedBackend\n",
    "from funcs.gdal.raster import *\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. configuration & global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ['HOME_DIR']\n",
    "\n",
    "gldas = \"5babae3f-c468-4e01-862e-8b201468e3b5\"\n",
    "gpm = \"ea0e86f3-9470-4e7e-a581-df85b4a7075d\"\n",
    "region = \"74e6f707-d5e9-4cbd-ae26-16ffa21a1d84\"\n",
    "variable = \"atmosphere_water__precipitation_mass_flux\"\n",
    "variable = \"land_surface_air__temperature\"\n",
    "\n",
    "ethiopia = BoundingBox(32.75418, 3.22206, 47.98942, 15.15943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. download the weather dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets(dataset_id, start_time, end_time):\n",
    "  if start_time is not None:\n",
    "    start_time = parse(start_time)\n",
    "  if end_time is not None:\n",
    "    end_time = parse(end_time)\n",
    "    \n",
    "  func = DcatReadFunc(dataset_id, start_time, end_time)\n",
    "  func.set_preferences({\"data\": \"array\"})\n",
    "  datasets = func.exec()['data']\n",
    "  return datasets\n",
    "\n",
    "def read_local_datasets(repr_file, resource_path):\n",
    "  drepr = DRepr.parse_from_file(repr_file)\n",
    "  files = glob.glob(resource_path)\n",
    "  \n",
    "  if len(files) == 1:\n",
    "    return outputs.ArrayBackend.from_drepr(drepr, files[0])\n",
    "  \n",
    "  ds = ShardedBackend(len(files))\n",
    "  for file in tqdm(files):\n",
    "    ds.add(outputs.ArrayBackend.from_drepr(drepr, file, ds.inject_class_id))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset = read_datasets(gldas, \"2011-09-01T00:00:00\", \"2011-09-01T03:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. crop the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful functions to convert datasets to rasters and convert them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + \"/examples/d3m/crop_bb.yml\", \"r\") as f:\n",
    "  crop_bb_conf = yaml.load(f)\n",
    "\n",
    "def dataset2raster(sm, variable):\n",
    "  rasters = []\n",
    "  for c in sm.c('mint:Variable').filter(outputs.FCondition(\"mint:standardName\", \"==\", variable)):\n",
    "    for raster_id, sc in c.group_by(\"mint-geo:raster\"):\n",
    "      # TODO: handle time properly\n",
    "      timestamp = sc.p(\"mint:timestamp\").as_ndarray([])\n",
    "      if timestamp.data.size != 1:\n",
    "        raise NotImplemented()\n",
    "      timestamp = timestamp.data[0]\n",
    "      \n",
    "      data = sc.p(\"rdf:value\").as_ndarray([sc.p(\"mint-geo:lat\"), sc.p(\"mint-geo:long\")])\n",
    "      gt_info = sm.get_record_by_id(raster_id)\n",
    "      gt = GeoTransform(x_0=gt_info.s(\"mint-geo:x_0\"),\n",
    "                        y_0=gt_info.s(\"mint-geo:y_0\"),\n",
    "                        dx=gt_info.s(\"mint-geo:dx\"), dy=gt_info.s(\"mint-geo:dy\"))\n",
    "      raster = Raster(data.data, gt, int(gt_info.s(\"mint-geo:epsg\")),\n",
    "             float(data.nodata.value) if data.nodata is not None else None)\n",
    "      raster.timestamp = timestamp\n",
    "      rasters.append(raster)\n",
    "  return rasters\n",
    "  \n",
    "def raster2dataset(r, variable):\n",
    "  global crop_bb_conf\n",
    "  reader = NPDictReader({\n",
    "    \"variable\": r.data,\n",
    "    \"lat\": r.get_center_latitude(),\n",
    "    \"long\": r.get_center_longitude(),\n",
    "    \"timestamp\": r.timestamp,\n",
    "    \"standard_name\": variable,\n",
    "    \"gt_x_0\": r.geotransform.x_0,\n",
    "    \"gt_y_0\": r.geotransform.y_0,\n",
    "    \"gt_dx\": r.geotransform.dx,\n",
    "    \"gt_dy\": r.geotransform.dy,\n",
    "    \"gt_epsg\": r.epsg,\n",
    "    \"gt_x_slope\": r.geotransform.x_slope,\n",
    "    \"gt_y_slope\": r.geotransform.y_slope,\n",
    "  })\n",
    "  resource_id = str(uuid.uuid4())\n",
    "  ReaderContainer.get_instance().set(resource_id, reader)\n",
    "  \n",
    "  conf = copy.deepcopy(crop_bb_conf)\n",
    "  conf['attributes']['variable']['missing_values'].append(r.nodata)\n",
    "  drepr = DRepr.parse(conf)\n",
    "  sm = outputs.ArrayBackend.from_drepr(drepr, resource_id)\n",
    "  ReaderContainer.get_instance().delete(resource_id)\n",
    "  return sm\n",
    "\n",
    "def raster2netcdf(r, variable, outfile):\n",
    "  lat = r.get_center_latitude()\n",
    "  long = r.get_center_longitude()\n",
    "  data = xr.DataArray(r.data, dims=('lat', 'long'), coords={'lat': lat, 'long': long})\n",
    "  data.attrs['standard_name'] = variable\n",
    "  data.attrs['_FillValue'] = r.nodata\n",
    "  data.attrs['missing_values'] = r.nodata\n",
    "  \n",
    "  ds = xr.Dataset({standard_name: data})  \n",
    "  ds.to_netcdf(outfile)\n",
    "  \n",
    "def dataset2netcdf(sm):\n",
    "  assert len(sm.c(\"mint:Variable\")) == 1\n",
    "  c = sm.c(\"mint:Variable\")[0]\n",
    "  if c.p(\"mint:Place\") is not None:\n",
    "    raise NotImplemented()\n",
    "    \n",
    "  standard_name = c.p(\"mint:standardName\").as_ndarray([]).data\n",
    "  assert standard_name.size == 1\n",
    "  standard_name = standard_name[0]\n",
    "\n",
    "  timestamp = c.p(\"mint:timestamp\").as_ndarray([]).data\n",
    "  assert timestamp.size == 1\n",
    "  timestamp = timestamp[0]\n",
    "  \n",
    "  groups = list(c.group_by(\"mint-geo:raster\"))\n",
    "  assert len(groups) == 1\n",
    "  gt = sm.get_record_by_id(groups[0][0])\n",
    "\n",
    "  val = c.p(\"rdf:value\").as_ndarray([c.p(\"mint-geo:lat\"), c.p(\"mint-geo:long\")])\n",
    "  data = val.data.reshape(1, *val.data.shape)\n",
    "  data = xr.DataArray(val.data.reshape(1, *val.data.shape), dims=('time', 'lat', 'long'), coords={\n",
    "    'lat': val.index_props[0], 'long': val.index_props[1], 'time': np.asarray([timestamp])\n",
    "  })\n",
    "  data.attrs['standard_name'] = standard_name\n",
    "  data.attrs['_FillValue'] = val.nodata.value\n",
    "  data.attrs['missing_values'] = val.nodata.value\n",
    "  \n",
    "  ds = xr.Dataset({\"variable\": data})\n",
    "  ds.attrs.update({\n",
    "    \"conventions\": \"CF-1.6\",\n",
    "    \"dx\": gt.s('mint-geo:dx'),\n",
    "    \"dy\": gt.s(\"mint-geo:dy\"),\n",
    "    \"epsg\": gt.s(\"mint-geo:epsg\"),\n",
    "    \"x_slope\": gt.s(\"mint-geo:x_slope\"),\n",
    "    \"y_slope\": gt.s(\"mint-geo:y_slope\"),\n",
    "    \"x_0\": gt.s(\"mint-geo:x_0\"),\n",
    "    \"y_0\": gt.s(\"mint-geo:y_0\")\n",
    "  })\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 crop the data by a bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters = []\n",
    "for raster in tqdm(dataset2raster(weather_dataset, variable)):\n",
    "  sr = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  sr.timestamp = raster.timestamp\n",
    "  filename = datetime.datetime.utcfromtimestamp(sr.timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "  sm = raster2dataset(sr, variable)\n",
    "  dataset2netcdf(sm).to_netcdf(HOME_DIR + f\"/data/gldas/{variable}/{filename}.nc4\")\n",
    "  subrasters.append(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug to see if the data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters[0].to_geotiff(HOME_DIR + \"/data/debug/small.tif\")\n",
    "raster.to_geotiff(HOME_DIR + \"/data/debug/full.tif\")  # incorrect because \n",
    "sm = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", \n",
    "                         HOME_DIR + f\"/data/gldas/{variable}/20110901000000.nc4\")\n",
    "a = dataset2raster(sm, variable)[0].data\n",
    "b = subrasters[0].data\n",
    "\n",
    "assert np.allclose(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 crop data by shapefiles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_array_to_shapefile(data, fname):\n",
    "  polygon = data[0]\n",
    "  if isinstance(polygon[0][0][0], (int, float)):\n",
    "    shape_type = 'Polygon'\n",
    "  else:\n",
    "    shape_type = 'MultiPolygon'\n",
    "\n",
    "  epsg = fiona.crs.from_epsg(data[1])\n",
    "  driver = \"ESRI Shapefile\"\n",
    "  polygon = {\n",
    "      'geometry': {\n",
    "          'type': shape_type,\n",
    "          'coordinates': polygon\n",
    "      },\n",
    "      'properties': {\n",
    "          'name': 'TempCroppingPolygon'\n",
    "      }\n",
    "  }\n",
    "  schema = {'geometry': shape_type, 'properties': {'name': 'str'}}\n",
    "  with fiona.open(fname, 'w', crs='+datum=WGS84 +ellps=WGS84 +no_defs +proj=longlat', driver=driver, schema=schema) as shapefile:\n",
    "    shapefile.write(polygon)\n",
    "    \n",
    "def create_shapefile(sm, dname, randomize: bool=False):\n",
    "  shape_files = []\n",
    "  random_id = str(uuid.uuid4())\n",
    "  for c in sm.c(\"mint:Place\"):\n",
    "    for r in c.iter_records():\n",
    "      polygon = sm.get_record_by_id(r.s('mint-geo:bounding')).s('rdf:value')\n",
    "      if randomize:\n",
    "        shape_file = HOME_DIR + f'/data/{dname}/{random_id}/{r.s(\"mint:region\").replace(\" \", \"-\")}.shp'\n",
    "      else:\n",
    "        shape_file = HOME_DIR + f'/data/{dname}/{r.s(\"mint:region\").replace(\" \", \"-\")}.shp'\n",
    "      Path(shape_file).parent.mkdir(exist_ok=True, parents=True)\n",
    "      shape_array_to_shapefile([polygon, 4326], shape_file)\n",
    "      shape_files.append({\n",
    "        \"file\": shape_file,\n",
    "        \"region\": r.s(\"mint:region\")\n",
    "      })\n",
    "  return shape_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dataset = read_datasets(region, None, None)\n",
    "shape_files = create_shapefile(region_dataset, 'debug/regions-random', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select datasets to crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = \"2011\"\n",
    "temp_dataset = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", \n",
    "                                   HOME_DIR + f\"/data/gldas/{variable}/{date_pattern}*.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = temp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sm.c(\"mint:Variable\"):\n",
    "  print(c.p(\"mint-geo:raster\"))\n",
    "  c.p(\"rdf:value\").as_ndarray([c.p(\"mint-geo:lat\"), c.p(\"mint-geo:long\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = []\n",
    "\n",
    "for raster in tqdm(dataset2raster(temp_dataset, variable)):\n",
    "  for shape_file in shape_files:\n",
    "    try:\n",
    "      sr = raster.crop(vector_file=shape_file['file'],\n",
    "                        resampling_algo=ReSample.BILINEAR,\n",
    "                        touch_cutline=True)\n",
    "    except:\n",
    "      raster.to_geotiff(HOME_DIR + f\"/data/debug/full.tif\")\n",
    "      print(shape_file['file'])\n",
    "#       sr.to_geotiff(HOME_DIR + f\"/data/debug/.tif\")\n",
    "      raise\n",
    "    temp_data.append({\n",
    "      \"data\": sr.data,\n",
    "      \"nodata\": sr.nodata,\n",
    "      \"region\": shape_file['region'],\n",
    "      \"timestamp\": raster.timestamp,\n",
    "      \"timestring\": datetime.datetime.utcfromtimestamp(raster.timestamp).strftime(\"%Y%m%d%H%M%S\"),\n",
    "      \"date\": datetime.datetime.utcfromtimestamp(raster.timestamp).strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "#     sr.to_geotiff(HOME_DIR + f\"/data/region_{shape_file['region'].replace(' ', '-')}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. calculate average daily temperature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = defaultdict(list)\n",
    "for d in tqdm(temp_data):\n",
    "  groups[(d['date'], d['region'])].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "i = 0\n",
    "for (date, region), meshs in groups.items():\n",
    "  i += 1\n",
    "#   print(i)\n",
    "#   if i != 1:\n",
    "#     continue\n",
    "  \n",
    "  n_obs_mesh = np.zeros(meshs[0]['data'].shape, dtype=np.int64)\n",
    "  sum_mesh = np.zeros(meshs[0]['data'].shape, dtype=np.float64)\n",
    "  for mesh in meshs:\n",
    "    d = mesh['data']\n",
    "    nodata = mesh['nodata']\n",
    "    \n",
    "    mask = (d != nodata).astype(np.int64)\n",
    "    # count the reported points and their values\n",
    "    n_obs_mesh += mask\n",
    "    sum_mesh += mask * d\n",
    "  \n",
    "  obs_mask = n_obs_mesh != 0\n",
    "  avg_daily_temp_mesh = sum_mesh[obs_mask] / n_obs_mesh[obs_mask]\n",
    "  avg_daily_temp = np.average(avg_daily_temp_mesh) - 273.15\n",
    "  \n",
    "  rows.append({\"date\": date, \"region\": region, \"avg_daily_temp (celsius)\": avg_daily_temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.sort(key=lambda x: (x['date'], x['region']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(HOME_DIR + f\"/data/outputs/avg_daily_temp.{date_pattern}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for file in sorted(glob.glob(HOME_DIR + \"/data/outputs/*.csv\")):\n",
    "#   print(file)\n",
    "  df = pd.read_csv(file, names=['date', 'region', 'avg_daily_temp (celsius)'], header=0)\n",
    "  rows += df.to_dict('records')\n",
    "\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.sort(key=lambda x: (x['date'], x['region']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "grouped = df.groupby('region')\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(grouped.ngroups/ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(32, 32), sharey=True)\n",
    "for (key, ax) in zip(grouped.groups.keys(), axes.flatten()):\n",
    "  grouped.get_group(key).plot(ax=ax, legend=True, title=key)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(HOME_DIR + \"/data/avg_daily_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
