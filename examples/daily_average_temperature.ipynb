{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, rdflib, pandas as pd, numpy as np, sys, os, random, math, fiona, uuid, copy, glob\n",
    "from osgeo import gdal, osr, gdal_array\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from typing import *\n",
    "from ruamel.yaml import YAML\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# next cell\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drepr version: 2.8\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(verbose=True)\n",
    "paths = [\"../\", \"/workspace/d-repr/pydrepr\", \"/home/rook/workspace/d-repr/pydrepr\"]\n",
    "for path in paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "from drepr import __version__, DRepr, outputs\n",
    "from drepr.executors.readers.reader_container import ReaderContainer\n",
    "from drepr.executors.readers.np_dict import NPDictReader\n",
    "print(\"drepr version:\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DcatReadFunc\n",
    "from funcs.trans_cropping_func import CroppingTransFunc\n",
    "from funcs.readers.dcat_read_func import ShardedClassID\n",
    "from funcs.gdal.raster import *\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. configuration & global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ['HOME_DIR']\n",
    "\n",
    "gldas = \"5babae3f-c468-4e01-862e-8b201468e3b5\"\n",
    "gpm = \"ea0e86f3-9470-4e7e-a581-df85b4a7075d\"\n",
    "region = \"74e6f707-d5e9-4cbd-ae26-16ffa21a1d84\"\n",
    "variable = \"atmosphere_water__precipitation_mass_flux\"\n",
    "variable = \"land_surface_air__temperature\"\n",
    "\n",
    "ethiopia = BoundingBox(32.75418, 3.22206, 47.98942, 15.15943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. download the weather dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_datasets(dataset_id, start_time, end_time):\n",
    "  if start_time is not None:\n",
    "    start_time = parse(start_time)\n",
    "  if end_time is not None:\n",
    "    end_time = parse(end_time)\n",
    "    \n",
    "  func = DcatReadFunc(dataset_id, start_time, end_time)\n",
    "  func.set_preferences({\"data\": \"array\"})\n",
    "  datasets = func.exec()['data']\n",
    "  return datasets\n",
    "\n",
    "def read_local_datasets(repr_file, resource_path):\n",
    "  drepr = DRepr.parse_from_file(repr_file)\n",
    "  datasets = []\n",
    "  for file in glob.glob(resource_path):\n",
    "    datasets.append(outputs.ArrayBackend.from_drepr(drepr, file))  \n",
    "  if len(datasets) > 1:\n",
    "    return ShardedBackend(datasets)\n",
    "  return datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-05 04:31:52,974 | funcs.readers.dcat_read_func | INFO - Overwrite GLDAS\n",
      "2020-03-05 04:31:52,975 | funcs.readers.dcat_read_func | INFO - Found key 'resource_repr'\n",
      "2020-03-05 04:31:52,975 | funcs.readers.dcat_read_func | INFO - Downloading 2928 resources ...\n",
      "2020-03-05 04:31:53,057 | funcs.readers.dcat_read_func | INFO - Download Complete. Skip 2928 and download 0 resources\n"
     ]
    }
   ],
   "source": [
    "weather_dataset = read_datasets(gldas, \"2016-01-01T00:00:00\", \"2017-01-01T00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. crop the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful functions to convert datasets to rasters and convert them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(HOME_DIR + \"/examples/d3m/crop_bb.yml\", \"r\") as f:\n",
    "  crop_bb_conf = yaml.load(f)\n",
    "\n",
    "def dataset2raster(sm, variable):\n",
    "  rasters = []\n",
    "  for c in sm.c('mint:Variable').filter(outputs.FCondition(\"mint:standardName\", \"==\", variable)):\n",
    "    for raster_id, sc in c.group_by(\"mint-geo:raster\"):\n",
    "      # TODO: handle time properly\n",
    "      timestamp = sc.p(\"mint:timestamp\").as_ndarray([])\n",
    "      if timestamp.data.size != 1:\n",
    "        raise NotImplemented()\n",
    "      timestamp = timestamp.data[0]\n",
    "      \n",
    "      data = sc.p(\"rdf:value\").as_ndarray([sc.p(\"mint-geo:lat\"), sc.p(\"mint-geo:long\")])\n",
    "      gt_info = sm.get_record_by_id(raster_id)\n",
    "      gt = GeoTransform(x_0=gt_info.s(\"mint-geo:x_0\"),\n",
    "                        y_0=gt_info.s(\"mint-geo:y_0\"),\n",
    "                        dx=gt_info.s(\"mint-geo:dx\"), dy=gt_info.s(\"mint-geo:dy\"))\n",
    "      raster = Raster(data.data, gt, int(gt_info.s(\"mint-geo:epsg\")),\n",
    "             float(data.nodata.value) if data.nodata is not None else None)\n",
    "      raster.timestamp = timestamp\n",
    "      rasters.append(raster)\n",
    "  return rasters\n",
    "  \n",
    "def raster2dataset(r, variable):\n",
    "  global crop_bb_conf\n",
    "  reader = NPDictReader({\n",
    "    \"variable\": r.data,\n",
    "    \"lat\": r.get_center_latitude(),\n",
    "    \"long\": r.get_center_longitude(),\n",
    "    \"timestamp\": r.timestamp,\n",
    "    \"standard_name\": variable,\n",
    "    \"gt_x_0\": r.geotransform.x_0,\n",
    "    \"gt_y_0\": r.geotransform.y_0,\n",
    "    \"gt_dx\": r.geotransform.dx,\n",
    "    \"gt_dy\": r.geotransform.dy,\n",
    "    \"gt_epsg\": r.epsg,\n",
    "    \"gt_x_slope\": r.geotransform.x_slope,\n",
    "    \"gt_y_slope\": r.geotransform.y_slope,\n",
    "  })\n",
    "  resource_id = str(uuid.uuid4())\n",
    "  ReaderContainer.get_instance().set(resource_id, reader)\n",
    "  \n",
    "  conf = copy.deepcopy(crop_bb_conf)\n",
    "  conf['attributes']['variable']['missing_values'].append(r.nodata)\n",
    "  drepr = DRepr.parse(conf)\n",
    "  sm = outputs.ArrayBackend.from_drepr(drepr, resource_id)\n",
    "  ReaderContainer.get_instance().delete(resource_id)\n",
    "  return sm\n",
    "\n",
    "def raster2netcdf(r, variable, outfile):\n",
    "  lat = r.get_center_latitude()\n",
    "  long = r.get_center_longitude()\n",
    "  data = xr.DataArray(r.data, dims=('lat', 'long'), coords={'lat': lat, 'long': long})\n",
    "  data.attrs['standard_name'] = variable\n",
    "  data.attrs['_FillValue'] = r.nodata\n",
    "  data.attrs['missing_values'] = r.nodata\n",
    "  \n",
    "  ds = xr.Dataset({standard_name: data})  \n",
    "  ds.to_netcdf(outfile)\n",
    "  \n",
    "def dataset2netcdf(sm):\n",
    "  assert len(sm.c(\"mint:Variable\")) == 1\n",
    "  c = sm.c(\"mint:Variable\")[0]\n",
    "  if c.p(\"mint:Place\") is not None:\n",
    "    raise NotImplemented()\n",
    "    \n",
    "  standard_name = c.p(\"mint:standardName\").as_ndarray([]).data\n",
    "  assert standard_name.size == 1\n",
    "  standard_name = standard_name[0]\n",
    "\n",
    "  timestamp = c.p(\"mint:timestamp\").as_ndarray([]).data\n",
    "  assert timestamp.size == 1\n",
    "  timestamp = timestamp[0]\n",
    "  \n",
    "  groups = list(c.group_by(\"mint-geo:raster\"))\n",
    "  assert len(groups) == 1\n",
    "  gt = sm.get_record_by_id(groups[0][0])\n",
    "\n",
    "  val = c.p(\"rdf:value\").as_ndarray([c.p(\"mint-geo:lat\"), c.p(\"mint-geo:long\")])\n",
    "  data = val.data.reshape(1, *val.data.shape)\n",
    "  data = xr.DataArray(val.data.reshape(1, *val.data.shape), dims=('time', 'lat', 'long'), coords={\n",
    "    'lat': val.index_props[0], 'long': val.index_props[1], 'time': np.asarray([timestamp])\n",
    "  })\n",
    "  data.attrs['standard_name'] = standard_name\n",
    "  data.attrs['_FillValue'] = val.nodata.value\n",
    "  data.attrs['missing_values'] = val.nodata.value\n",
    "  \n",
    "  ds = xr.Dataset({\"variable\": data})\n",
    "  ds.attrs.update({\n",
    "    \"conventions\": \"CF-1.6\",\n",
    "    \"dx\": gt.s('mint-geo:dx'),\n",
    "    \"dy\": gt.s(\"mint-geo:dy\"),\n",
    "    \"epsg\": gt.s(\"mint-geo:epsg\"),\n",
    "    \"x_slope\": gt.s(\"mint-geo:x_slope\"),\n",
    "    \"y_slope\": gt.s(\"mint-geo:y_slope\"),\n",
    "    \"x_0\": gt.s(\"mint-geo:x_0\"),\n",
    "    \"y_0\": gt.s(\"mint-geo:y_0\")\n",
    "  })\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 crop the data by a bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters = []\n",
    "for raster in tqdm(dataset2raster(weather_dataset, variable)):\n",
    "  sr = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  sr.timestamp = raster.timestamp\n",
    "  filename = datetime.datetime.utcfromtimestamp(sr.timestamp).strftime(\"%Y%m%d%H%M%S\")\n",
    "  sm = raster2dataset(sr, variable)\n",
    "  dataset2netcdf(sm).to_netcdf(HOME_DIR + f\"/data/gldas/{variable}/{filename}.nc4\")\n",
    "  subrasters.append(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug to see if the data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters[-1].to_geotiff(HOME_DIR + \"/data/debug/small.tif\")\n",
    "raster.to_geotiff(HOME_DIR + \"/data/debug/full.tif\")\n",
    "sm = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", HOME_DIR + f\"/data/gldas/{variable}/201109*.nc4\")\n",
    "a = dataset2raster(sm, variable)[0].data\n",
    "b = subrasters[-1].data\n",
    "\n",
    "assert np.allclose(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 crop data by shapefiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_array_to_shapefile(data, fname):\n",
    "  polygon = data[0]\n",
    "  if isinstance(polygon[0][0][0], (int, float)):\n",
    "    shape_type = 'Polygon'\n",
    "  else:\n",
    "    shape_type = 'MultiPolygon'\n",
    "\n",
    "  epsg = fiona.crs.from_epsg(data[1])\n",
    "  driver = \"ESRI Shapefile\"\n",
    "  polygon = {\n",
    "      'geometry': {\n",
    "          'type': shape_type,\n",
    "          'coordinates': polygon\n",
    "      },\n",
    "      'properties': {\n",
    "          'name': 'TempCroppingPolygon'\n",
    "      }\n",
    "  }\n",
    "  schema = {'geometry': shape_type, 'properties': {'name': 'str'}}\n",
    "  with fiona.open(fname, 'w', crs='+datum=WGS84 +ellps=WGS84 +no_defs +proj=longlat', driver=driver, schema=schema) as shapefile:\n",
    "    shapefile.write(polygon)\n",
    "    \n",
    "def create_shapefile(sm, dname):\n",
    "  shape_files = []\n",
    "  for c in sm.c(\"mint:Place\"):\n",
    "    for r in c.iter_records():\n",
    "      polygon = sm.get_record_by_id(r.s('mint-geo:bounding')).s('rdf:value')\n",
    "      shape_file = HOME_DIR + f'/data/{dname}/{r.s(\"mint:region\").replace(\" \", \"-\")}.shp'\n",
    "      shape_array_to_shapefile([polygon, 4326], shape_file)\n",
    "      shape_files.append({\n",
    "        \"file\": shape_file,\n",
    "        \"region\": r.s(\"mint:region\")\n",
    "      })\n",
    "  return shape_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-05 03:11:27,490 | funcs.readers.dcat_read_func | INFO - Found key 'dataset_repr'\n",
      "2020-03-05 03:11:27,491 | funcs.readers.dcat_read_func | INFO - Downloading 1 resources ...\n",
      "2020-03-05 03:11:27,493 | funcs.readers.dcat_read_func | INFO - Download Complete. Skip 1 and download 0 resources\n"
     ]
    }
   ],
   "source": [
    "region_dataset = read_datasets(region, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_files = create_shapefile(region_dataset, 'debug/regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_dataset = read_local_datasets(HOME_DIR + \"/examples/d3m/gldas.crop.yml\", \n",
    "                                      HOME_DIR + f\"/data/gldas/{variable}/201109*.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798528d8bff746dca0a7ef3e96288f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "103\n",
      "501\n",
      "6\n",
      "4\n",
      "195\n",
      "67\n",
      "101\n",
      "251\n",
      "169\n",
      "535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_rasters = []\n",
    "\n",
    "for raster in tqdm(dataset2raster(cropped_dataset, variable)):\n",
    "  raster.to_geotiff(HOME_DIR + f\"/data/region_full.tif\")\n",
    "  for shape_file in shape_files:\n",
    "    sr = raster.crop(vector_file=shape_file['file'],\n",
    "                      resampling_algo=ReSample.BILINEAR,\n",
    "                      touch_cutline=True)\n",
    "    region_rasters.append(sr)\n",
    "    sr.to_geotiff(HOME_DIR + f\"/data/region_{shape_file['region'].replace(' ', '-')}.tif\")\n",
    "    print(np.sum(sr.data != -9999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = raster2dataset(rasters[0], variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2netcdf(sm).to_netcdf(HOME_DIR + \"/data/tmp_out/test.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in rasters:\n",
    "  raster.to_geotiff(HOME_DIR + \"/data/full.tif\")\n",
    "  ethiopia_raster = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  ethiopia_raster.to_geotiff(HOME_DIR + \"/data/small.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = raster2dataset(ethiopia_raster, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "defaultdict(<class 'list'>, {'rdf:value': [0], 'mint-geo:lat': [1], 'mint-geo:long': [2], 'mint:standardName': [3], 'mint-geo:raster': [4], 'mint:timestamp': []})\n",
      "defaultdict(<class 'list'>, {'rdf:value': [0], 'mint-geo:lat': [1], 'mint-geo:long': [2], 'mint:standardName': [3], 'mint-geo:raster': [4], 'mint:timestamp': []})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-ddbb54719006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset2netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-234-26a41d494d80>\u001b[0m in \u001b[0;36mdataset2netcdf\u001b[0;34m(sm)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:standardName\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rdf:value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint-geo:lat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint-geo:long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         data = xr.DataArray(val.data, dims=('lat', 'long', 'time'), coords={\n",
      "\u001b[0;32m/workspace/d-repr/pydrepr/drepr/outputs/array_backend/array_class.py\u001b[0m in \u001b[0;36mgroup_by\u001b[0;34m(self, predicate)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key of group_by operator cannot be a list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# p = self.p(predicate)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred2attrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset2netcdf(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
