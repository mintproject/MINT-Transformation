{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt, rdflib, pandas as pd, numpy as np, sys, os, random, math, fiona, uuid, copy\n",
    "from osgeo import gdal, osr, gdal_array\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from typing import *\n",
    "from ruamel.yaml import YAML\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# next cell\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drepr version: 2.8\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(verbose=True)\n",
    "paths = [\"../\", \"/workspace/d-repr/pydrepr\"]\n",
    "for path in paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "from drepr import __version__, DRepr, outputs\n",
    "from drepr.executors.readers.reader_container import ReaderContainer\n",
    "from drepr.executors.readers.np_dict import NPDictReader\n",
    "print(\"drepr version:\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import DcatReadFunc\n",
    "from funcs.readers.dcat_read_func import ShardedClassID\n",
    "from funcs.gdal.raster import *\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**configuration & global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ['HOME_DIR']\n",
    "\n",
    "gldas = \"5babae3f-c468-4e01-862e-8b201468e3b5\"\n",
    "gpm = \"ea0e86f3-9470-4e7e-a581-df85b4a7075d\"\n",
    "variable = \"atmosphere_water__precipitation_mass_flux\"\n",
    "variable = \"land_surface_air__temperature\"\n",
    "\n",
    "ethiopia = BoundingBox(32.75418, 3.22206, 47.98942, 15.15943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download the weather dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets(dataset_id, start_time, end_time):\n",
    "  func = DcatReadFunc(dataset_id, parse(start_time), parse(end_time))\n",
    "  func.set_preferences({\"data\": \"array\"})\n",
    "  datasets = func.exec()['data']\n",
    "  return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-04 01:13:20,959 | funcs.readers.dcat_read_func | INFO - Overwrite GLDAS\n",
      "2020-03-04 01:13:20,960 | funcs.readers.dcat_read_func | DEBUG - Found key 'resource_repr'\n",
      "2020-03-04 01:13:20,961 | funcs.readers.dcat_read_func | DEBUG - Downloading 8 resources ...\n",
      "2020-03-04 01:13:20,961 | funcs.readers.dcat_read_func | DEBUG - Skipping resource 7e945d34-73e3-46b8-aa5d-17d5e61a23da, found in cache\n",
      "2020-03-04 01:13:20,962 | funcs.readers.dcat_read_func | DEBUG - Skipping resource a4d67e6f-3f5b-475a-9ac2-a27dd527f4a2, found in cache\n",
      "2020-03-04 01:13:20,963 | funcs.readers.dcat_read_func | DEBUG - Skipping resource 0f4a8ce6-fcab-450a-96bd-29d1819c4cf1, found in cache\n",
      "2020-03-04 01:13:20,964 | funcs.readers.dcat_read_func | DEBUG - Skipping resource a404e94b-78ef-49d8-9f73-38b2368b3a7c, found in cache\n",
      "2020-03-04 01:13:20,966 | funcs.readers.dcat_read_func | DEBUG - Skipping resource c7764c4a-0936-4513-b359-e2f6deafe267, found in cache\n",
      "2020-03-04 01:13:20,968 | funcs.readers.dcat_read_func | DEBUG - Skipping resource 3c6e05d1-c220-48f5-b6b9-fd5d3271f0d5, found in cache\n",
      "2020-03-04 01:13:20,970 | funcs.readers.dcat_read_func | DEBUG - Skipping resource ef3475d8-c56d-4398-a65e-78e33cb1588b, found in cache\n",
      "2020-03-04 01:13:20,971 | funcs.readers.dcat_read_func | DEBUG - Skipping resource d105a134-e908-4d3c-9670-6cf9f1758e9a, found in cache\n",
      "2020-03-04 01:13:20,972 | funcs.readers.dcat_read_func | DEBUG - Download Complete\n"
     ]
    }
   ],
   "source": [
    "weather_dataset = read_datasets(gldas, \"2011-09-01T00:00:00\", \"2011-09-01T23:59:59\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**crop the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful functions to convert datasets to rasters and convert them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + \"/examples/d3m/crop_bb.yml\", \"r\") as f:\n",
    "  crop_bb_conf = yaml.load(f)\n",
    "\n",
    "def dataset2raster(sm, variable):\n",
    "  rasters = []\n",
    "  for c in sm.c('mint:Variable').filter(outputs.FCondition(\"mint:standardName\", \"==\", variable)):\n",
    "    for raster_id, sc in c.group_by(\"mint-geo:raster\"):\n",
    "      # TODO: handle time properly\n",
    "      timestamp = sc.p(\"mint:timestamp\").as_ndarray([])\n",
    "      if timestamp.data.size != 1:\n",
    "        raise NotImplemented()\n",
    "      timestamp = timestamp.data[0]\n",
    "      \n",
    "      data = sc.p(\"rdf:value\").as_ndarray([sc.p(\"mint-geo:lat\"), sc.p(\"mint-geo:long\")])\n",
    "      gt_info = sm.get_record_by_id(raster_id)\n",
    "      gt = GeoTransform(x_0=gt_info.s(\"mint-geo:x_0\"),\n",
    "                        y_0=gt_info.s(\"mint-geo:y_0\"),\n",
    "                        dx=gt_info.s(\"mint-geo:dx\"), dy=gt_info.s(\"mint-geo:dy\"))\n",
    "      raster = Raster(data.data, gt, int(gt_info.s(\"mint-geo:epsg\")),\n",
    "             float(data.nodata.value) if data.nodata is not None else None)\n",
    "      raster.timestamp = timestamp\n",
    "      rasters.append(raster)\n",
    "  return rasters\n",
    "  \n",
    "def raster2dataset(r, variable):\n",
    "  global crop_bb_conf\n",
    "  reader = NPDictReader({\n",
    "    \"variable\": r.data,\n",
    "    \"lat\": r.get_center_latitude(),\n",
    "    \"long\": r.get_center_longitude(),\n",
    "    \"timestamp\": r.timestamp,\n",
    "    \"standard_name\": variable,\n",
    "    \"gt_x_0\": r.geotransform.x_0,\n",
    "    \"gt_y_0\": r.geotransform.y_0,\n",
    "    \"gt_dx\": r.geotransform.dx,\n",
    "    \"gt_dy\": r.geotransform.dy,\n",
    "    \"gt_epsg\": r.epsg,\n",
    "    \"gt_x_slope\": r.geotransform.x_slope,\n",
    "    \"gt_y_slope\": r.geotransform.y_slope,\n",
    "  })\n",
    "  resource_id = str(uuid.uuid4())\n",
    "  ReaderContainer.get_instance().set(resource_id, reader)\n",
    "  \n",
    "  conf = copy.deepcopy(crop_bb_conf)\n",
    "  conf['attributes']['variable']['missing_values'].append(r.nodata)\n",
    "  drepr = DRepr.parse(conf)\n",
    "  sm = outputs.ArrayBackend.from_drepr(drepr, resource_id)\n",
    "  ReaderContainer.get_instance().delete(resource_id)\n",
    "  return sm\n",
    "\n",
    "def raster2netcdf(r, variable, outfile):\n",
    "  lat = r.get_center_latitude()\n",
    "  long = r.get_center_longitude()\n",
    "  data = xr.DataArray(r.data, dims=('lat', 'long'), coords={'lat': lat, 'long': long})\n",
    "  data.attrs['standard_name'] = variable\n",
    "  data.attrs['_FillValue'] = r.nodata\n",
    "  data.attrs['missing_values'] = r.nodata\n",
    "  \n",
    "  ds = xr.Dataset({standard_name: data})  \n",
    "  ds.to_netcdf(outfile)\n",
    "  \n",
    "def dataset2netcdf(sm):\n",
    "  datasets = {}\n",
    "  for c in sm.c(\"mint:Variable\"):\n",
    "    if c.p(\"mint:Place\") is not None:\n",
    "      raise NotImplemented()\n",
    "\n",
    "    for standard_name, sc1 in c.group_by(\"mint:standardName\"):\n",
    "      for time, sc2 in sc1.group_by(\"mint:timestamp\"):\n",
    "        val = sc2.p(\"rdf:value\").as_ndarray([sc2.p(\"mint-geo:lat\"), sc2.p(\"mint-geo:long\")])\n",
    "        data = val.data.reshape(1, *val.data.shape)\n",
    "        data = xr.DataArray(val.data.reshape(1, *val.data.shape), dims=('time', 'lat', 'long'), coords={\n",
    "          'lat': val.index_props[0], 'long': val.index_props[1], 'time': np.asarray([time])\n",
    "        })\n",
    "        data.attrs['standard_name'] = standard_name\n",
    "        data.attrs['_FillValue'] = val.nodata.value\n",
    "        data.attrs['missing_values'] = val.nodata.value\n",
    "        \n",
    "        assert standard_name not in datasets\n",
    "        datasets[standard_name] = data\n",
    "  ds = xr.Dataset(datasets)\n",
    "  ds.attrs.update({\n",
    "    \"conventions\": \"CF-1.6\"\n",
    "  })\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop the data by a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters = dataset2raster(weather_dataset, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = rasters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Raster' object has no attribute 'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-c42264d65e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Raster' object has no attribute 'timestamp'"
     ]
    }
   ],
   "source": [
    "sr.timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1970, 3, 13, 0, 21)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(sr.timestamp)#.strftime(\"%Y-%M-%D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrasters = []\n",
    "for raster in rasters:\n",
    "  sr = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  sr.timestamp = raster.timestamp\n",
    "  sm = raster2dataset(sr, variable)\n",
    "  dataset2netcdf(sm).to_netcdf(HOME_DIR + f\"/data/tmp_out/{int(sr.timestamp)}.nc4\")\n",
    "  subrasters.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = raster2dataset(rasters[0], variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2netcdf(sm).to_netcdf(HOME_DIR + \"/data/tmp_out/test.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in rasters:\n",
    "  raster.to_geotiff(HOME_DIR + \"/data/full.tif\")\n",
    "  ethiopia_raster = raster.crop(bounds=ethiopia, resampling_algo=ReSample.BILINEAR)\n",
    "  ethiopia_raster.to_geotiff(HOME_DIR + \"/data/small.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = raster2dataset(ethiopia_raster, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "defaultdict(<class 'list'>, {'rdf:value': [0], 'mint-geo:lat': [1], 'mint-geo:long': [2], 'mint:standardName': [3], 'mint-geo:raster': [4], 'mint:timestamp': []})\n",
      "defaultdict(<class 'list'>, {'rdf:value': [0], 'mint-geo:lat': [1], 'mint-geo:long': [2], 'mint:standardName': [3], 'mint-geo:raster': [4], 'mint:timestamp': []})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-ddbb54719006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset2netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-234-26a41d494d80>\u001b[0m in \u001b[0;36mdataset2netcdf\u001b[0;34m(sm)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:standardName\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint:timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rdf:value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint-geo:lat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mint-geo:long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         data = xr.DataArray(val.data, dims=('lat', 'long', 'time'), coords={\n",
      "\u001b[0;32m/workspace/d-repr/pydrepr/drepr/outputs/array_backend/array_class.py\u001b[0m in \u001b[0;36mgroup_by\u001b[0;34m(self, predicate)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key of group_by operator cannot be a list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# p = self.p(predicate)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred2attrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset2netcdf(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintdt",
   "language": "python",
   "name": "mintdt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
