{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import ujson, csv, time\n",
    "from dtran import Pipeline, IFunc, ArgType\n",
    "from funcs import ReadFunc, WriteFunc\n",
    "from pydrepr import Graph\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a demo with an example flow for the MINT transformation pipeline\n",
    "\n",
    "This notebook presents an abstraction of the MINT Data Transformations architecture and walks thourgh an example through the pipeline we are suggesting; we show a simple CSV --(unit transformations)--> CSV flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Semantic) Description of a component (reader/writer/transformation functions)\n",
    "\n",
    "Providing semantic description of a function may allow constructing the pipeline semi-automatically. There are two levels of a semantic description:\n",
    "\n",
    "1. Providing definition for inputs/outputs of a component: it enables input data validation and compatibility checking between connected components.\n",
    "2. Providing definition of the purpose of the component: it allows us to construct the transformation pipeline based on some specification from users\n",
    "\n",
    "```python\n",
    "class IFunc(abc.ABC):\n",
    "    id: str = \"\"\n",
    "        \n",
    "    # level 2\n",
    "    description = None\n",
    "    \n",
    "    # level 1\n",
    "    inputs: Dict[str, ArgType] = {}\n",
    "    outputs: Dict[str, ArgType] = {}\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the inputs are correct or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def exec(self) -> dict:\n",
    "        \"\"\"\n",
    "        Execute the transformation function and return the result\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the semantic description of the interface (control plane) between all of the components in the pipeline (Reader/Writer/Transformation Adapters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's an example flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: Reader Adapter\n",
    "\n",
    "An instance of a reader adapter can be used as an entry point in the pipeline. It reads an input file `input.csv` file and a `input.yaml` file describing the D-REPR layout of this file. The data are representated in general way in a python object (Graph or NumPY array) and will be used in the next steps in the pipeline.\n",
    "\n",
    "```python\n",
    "class ReadFunc(IFunc):\n",
    "    id = \"read_func\"\n",
    "    inputs = {\n",
    "        \"repr_file\": ArgType.FilePath, \n",
    "        \"resources\": ArgType.String\n",
    "    }\n",
    "    outputs = {\"data\": ArgType.Graph(None)}\n",
    "    \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: Transformation Adapter\n",
    "\n",
    "An instance of a transformation adapter does not materialize the data into an output, it just reproduces the data, transformaing its content (the actual data) and performing the needed configrations in the 'control plane' parameters.\n",
    "Given the pythonic object (graph) representing the data and the rest of the needed configurations in the calling API ('control plane' parameters) we can re-construct the data in a new pythonic object (graph) and prepare it for the next steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_graph_obj --> python_graph_obj*\n",
    "class UnitTransFunc(IFunc):\n",
    "    id = \"unit_trans\"\n",
    "    inputs = {\"graph\": ArgType.Graph(None)}\n",
    "    outputs = {\"graph\": ArgType.Graph(None)}\n",
    "    \n",
    "    def __init__(self, graph: Graph):\n",
    "        self.graph = graph\n",
    "        \n",
    "    def exec(self):\n",
    "        return {\n",
    "            \"graph\": self.graph\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: Writer Adapter\n",
    "\n",
    "An instance of a writer adapter can be used as an exit point in the pipeline. It writes an output file `output.csv` based on a given `output.yaml` (D-REPR layout) and an additional configuration file `output.config`\n",
    "\n",
    "```python\n",
    "class WriteFunc(IFunc):\n",
    "    id = \"write_func\"\n",
    "    inputs = {\"graph\": ArgType.Graph(None), \"repr_file\": ArgType.FilePath, \"main_class\": ArgType.String}\n",
    "    outputs = {\"resources\": ArgType.String}\n",
    "\n",
    "    def __init__(self, graph: Graph, repr_file: str, main_class: str):\n",
    "        self.graph = graph\n",
    "        self.main_class = main_class\n",
    "\n",
    "        self.repr = Repr.from_file(repr_file)\n",
    "\n",
    "    def exec(self) -> dict:\n",
    "        all_data_rows = []\n",
    "        for rid, resource in self.repr.iter_resources():\n",
    "            if resource[\"type\"] == \"csv\":\n",
    "                all_data_rows = self.tabularize_data()\n",
    "                self._dump_to_csv(all_data_rows)\n",
    "            elif resource[\"type\"] == \"json\":\n",
    "                all_data_rows = self.tabularize_data()\n",
    "                self._dump_to_json(all_data_rows)\n",
    "        return {\"data\": all_data_rows}\n",
    "    ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-da05186be059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mUnitTransFunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWriteFunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m ])\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Projects\\MINT-Transformation\\dtran\\pipeline.py\u001b[0m in \u001b[0;36mexec\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0margname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunc_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"{func_cls.id}__{i}__{argname}\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\MINT-Transformation\\funcs\\write_func.py\u001b[0m in \u001b[0;36mexec\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mall_data_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtabularize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mWriteFunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dump_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mWriteFunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dump_to_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\MINT-Transformation\\funcs\\write_func.py\u001b[0m in \u001b[0;36m_dump_to_csv\u001b[1;34m(tabular_rows, attr_names, file_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtabular_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36mwriteheader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriteheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    ReadFunc.I.repr_file: \"./examples/s01_ethiopia_commodity_price.yml\",\n",
    "    ReadFunc.I.resources: \"./examples/s01_ethiopia_commodity_price.csv\",\n",
    "    WriteFunc.I.main_class: \"qb:Observation\",\n",
    "    WriteFunc.I.output_file: \"./examples/s01_ethiopia_commodity_price_write.csv\",\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ReadFunc,\n",
    "    UnitTransFunc,\n",
    "    WriteFunc\n",
    "], wired=[\n",
    "    ReadFunc.O.data == UnitTransFunc.I.graph,\n",
    "    UnitTransFunc.O.graph == WriteFunc.I.graph\n",
    "])\n",
    "outputs = pipeline.exec(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eg:unit': '$/bbl',\n",
       "  'rdf:value': '1.630000114440',\n",
       "  'rdfs:label': 'Crude oil, average($/bbl)',\n",
       "  'eg:recorded_at': '1960-01-01'},\n",
       " {'eg:unit': '$/bbl',\n",
       "  'rdf:value': '1.630000114440',\n",
       "  'rdfs:label': 'Crude oil, average($/bbl)',\n",
       "  'eg:recorded_at': '1960-02-01'},\n",
       " {'eg:unit': '$/bbl',\n",
       "  'rdf:value': '1.630000114440',\n",
       "  'rdfs:label': 'Crude oil, average($/bbl)',\n",
       "  'eg:recorded_at': '1960-03-01'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[str(WriteFunc.O._2.data)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
