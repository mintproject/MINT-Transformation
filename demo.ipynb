{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson, csv, time\n",
    "from dtran import Pipeline, IFunc, ArgType\n",
    "from funcs import ReadFunc, WriteFuncGraph, UnitTransFunc\n",
    "from pydrepr import Graph\n",
    "\n",
    "DEMO_CSV_INPUT_FILE = \"./examples/s01_ethiopia_commodity_price.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a demo with an example flow for the MINT transformation pipeline\n",
    "\n",
    "This notebook presents an abstraction of the MINT Data Transformations architecture and walks thourgh an example through the pipeline we are suggesting; we show a simple CSV --(unit transformations)--> CSV flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Indicator Code              IndicatorName   Unit Frequency     Date  Value\n",
      "0    CRUDE_PETRO  Crude oil, average($/bbl)  $/bbl         M  1960M01   1.63\n",
      "1    CRUDE_PETRO  Crude oil, average($/bbl)  $/bbl         M  1960M02   1.63\n",
      "2    CRUDE_PETRO  Crude oil, average($/bbl)  $/bbl         M  1960M03   1.63\n",
      "3    CRUDE_PETRO  Crude oil, average($/bbl)  $/bbl         M  1960M04   1.63\n",
      "4    CRUDE_PETRO  Crude oil, average($/bbl)  $/bbl         M  1960M05   1.63\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows of the input demo file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(DEMO_CSV_INPUT_FILE)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Semantic) Description of a component (reader/writer/transformation functions)\n",
    "\n",
    "Providing semantic description of a function may allow constructing the pipeline semi-automatically. There are two levels of a semantic description:\n",
    "\n",
    "1. Providing definition for inputs/outputs of a component: it enables input data validation and compatibility checking between connected components.\n",
    "2. Providing definition of the purpose of the component: it allows us to construct the transformation pipeline based on some specification from users\n",
    "\n",
    "```python\n",
    "class IFunc(abc.ABC):\n",
    "    id: str = \"\"\n",
    "        \n",
    "    # level 2\n",
    "    description = None\n",
    "    \n",
    "    # level 1\n",
    "    inputs: Dict[str, ArgType] = {}\n",
    "    outputs: Dict[str, ArgType] = {}\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the inputs are correct or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def exec(self) -> dict:\n",
    "        \"\"\"\n",
    "        Execute the transformation function and return the result\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the semantic description of the interface (control plane) between all of the components in the pipeline (Reader/Writer/Transformation Adapters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's an example flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: Reader Adapter\n",
    "\n",
    "An instance of a reader adapter can be used as an entry point in the pipeline. It reads an input file `input.csv` file and a `input.yaml` file describing the D-REPR layout of this file. The data are representated in general way in a python object (Graph or NumPY array) and will be used in the next steps in the pipeline.\n",
    "\n",
    "```python\n",
    "class ReadFunc(IFunc):\n",
    "    id = \"read_func\"\n",
    "    inputs = {\n",
    "        \"repr_file\": ArgType.FilePath, \n",
    "        \"resources\": ArgType.String\n",
    "    }\n",
    "    outputs = {\"data\": ArgType.Graph(None)}\n",
    "    \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: Transformation Adapter\n",
    "\n",
    "An instance of a transformation adapter does not materialize the data into an output, it just reproduces the data, transformaing its content (the actual data) and performing the needed configrations in the 'control plane' parameters.\n",
    "Given the pythonic object (graph) representing the data and the rest of the needed configurations in the calling API ('control plane' parameters) we can re-construct the data in a new pythonic object (graph) and prepare it for the next steps in the pipeline.\n",
    "\n",
    "```python\n",
    "class TransFunc(IFunc):\n",
    "    id = \"trans_func\"\n",
    "    inputs = {\"graph\": ArgType.Graph(None)}\n",
    "    outputs = {\"graph\": ArgType.Graph(None)}\n",
    "    \n",
    "    def __init__(self, graph: Graph):\n",
    "        self.graph = graph\n",
    "        \n",
    "    def exec(self):\n",
    "        for node in self.graph.nodes:\n",
    "            # do something for each node\n",
    "        return {\n",
    "            \"graph\": self.graph\n",
    "        }\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: Writer Adapter\n",
    "\n",
    "An instance of a writer adapter can be used as an exit point in the pipeline. It writes an output file `output.csv` based on a given `output.yaml` (D-REPR layout) and an additional configuration file `output.config`\n",
    "\n",
    "```python\n",
    "class WriteFuncGraph(IFunc):\n",
    "    id = \"write_func_graph\"\n",
    "    inputs = {\"graph\": ArgType.Graph(None), \"repr_file\": ArgType.FilePath, \"main_class\": ArgType.String}\n",
    "    outputs = {\"resources\": ArgType.String}\n",
    "\n",
    "    def __init__(self, graph: Graph, repr_file: str, main_class: str):\n",
    "        self.graph = graph\n",
    "        self.main_class = main_class\n",
    "\n",
    "        self.repr = Repr.from_file(repr_file)\n",
    "\n",
    "    def exec(self) -> dict:\n",
    "        all_data_rows = []\n",
    "        for rid, resource in self.repr.iter_resources():\n",
    "            if resource[\"type\"] == \"csv\":\n",
    "                all_data_rows = self.tabularize_data()\n",
    "                self._dump_to_csv(all_data_rows)\n",
    "            elif resource[\"type\"] == \"json\":\n",
    "                all_data_rows = self.tabularize_data()\n",
    "                self._dump_to_json(all_data_rows)\n",
    "        return {\"data\": all_data_rows}\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "Main idea for generalizing writers: \n",
    "- Implement a random indexer for each file type. \n",
    "- Design a configuration file to specify dimensional mapping between different attributes \n",
    "    1. extend Binh's representation file\n",
    "    2. redesign a new configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_CSV_OUTPUT_FILE = \"./examples/s01_ethiopia_commodity_price_write.csv\"\n",
    "\n",
    "inputs = {\n",
    "    ReadFunc.I.repr_file: \"./examples/s01_ethiopia_commodity_price.yml\",\n",
    "    ReadFunc.I.resources: DEMO_CSV_INPUT_FILE,\n",
    "    UnitTransFunc.I.unit_value: \"rdf:value\",\n",
    "    UnitTransFunc.I.unit_label: \"eg:unit\",\n",
    "    UnitTransFunc.I.unit_desired: \"$/liter\",\n",
    "    WriteFuncGraph.I.main_class: \"qb:Observation\",\n",
    "    WriteFuncGraph.I.output_file: DEMO_CSV_OUTPUT_FILE\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ReadFunc,\n",
    "    UnitTransFunc,\n",
    "    WriteFuncGraph\n",
    "], wired=[\n",
    "    ReadFunc.O.data == UnitTransFunc.I.graph,\n",
    "    UnitTransFunc.O.graph == WriteFuncGraph.I.graph\n",
    "])\n",
    "outputs = pipeline.exec(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eg:unit                 rdfs:label  rdf:value eg:recorded_at\n",
      "0  $/liter  Crude oil, average($/bbl)   0.010252     1960-01-01\n",
      "1  $/liter  Crude oil, average($/bbl)   0.010252     1960-02-01\n",
      "2  $/liter  Crude oil, average($/bbl)   0.010252     1960-03-01\n",
      "3  $/liter  Crude oil, average($/bbl)   0.010252     1960-04-01\n",
      "4  $/liter  Crude oil, average($/bbl)   0.010252     1960-05-01\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows of the output demo file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(DEMO_CSV_OUTPUT_FILE)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
