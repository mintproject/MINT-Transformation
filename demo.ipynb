{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a demo with an example flow for the MINT transformation pipeline\n",
    "\n",
    "This notebook presents an abstraction of the MINT Data Transformations architecture and walks thourgh an example through the pipeline we are suggesting; we show a simple CSV --(unit transformations)--> CSV flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the semantic description of the interface (control plane) between all of the components in the pipeline (Reader/Writer/Transformation Adapters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' These are the general data types we want to support\n",
    "in the interface between the different adapters. '''\n",
    "enum ArgType:\n",
    "    FilePath(Format),\n",
    "    Graph(SemanticModel),\n",
    "    NumpyArray(Schema),\n",
    "    String,\n",
    "    Number,\n",
    "    Boolean,\n",
    "    DateTime\n",
    "\n",
    "''' Each transformation function represents a transformation\n",
    "operation (or a transformation library) we would like to encode\n",
    "(i.e. Unit-Transformation, GDAL, PIHM2Cycles, etc...) '''\n",
    "class TransFunc:\n",
    "    id: Str\n",
    "    description: Str\n",
    "    inputs: Dict[str, Optional[ArgType]];\n",
    "    output: Dict[str, Optional[ArgType]];\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        pass\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def exec(self) -> dict:\n",
    "        pass\n",
    "\n",
    "\n",
    "''' This class inherits the general transformation function class\n",
    "and specifies the input and output interfaces of this 'Adapter'\n",
    "(its specification over the 'control plane') '''\n",
    "class UnitTransformation(TransFunc):\n",
    "    id: Str = \"unit_transformation\"\n",
    "    description: Str = \"\"\n",
    "    inputs = {\n",
    "        \"conf\": ArgType.FilePath,\n",
    "        \"source_unit\": ArgType.String,\n",
    "        \"data\": ArgType.Graph\n",
    "    }\n",
    "    outputs = {\n",
    "        \"data\": ArgType.Graph\n",
    "    }\n",
    "\n",
    "    def __init__(self, config, data, target_unit, source_unit):\n",
    "        pass\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def exec(self) -> dict:\n",
    "        pass\n",
    "\n",
    "''' The pipeline instance is a collection of TransFunc instances\n",
    "which eventually would be concatenated (in the 'control plane') '''\n",
    "pipeline = [\n",
    "    ReaderPDF,\n",
    "    UnitTransformation\n",
    "]\n",
    "args = {\n",
    "    \"<id>_<index>_<arg_name>\": <value>\n",
    "}\n",
    "args['unit_transformation_1_arg_name'] = None\n",
    "pipeline.exec(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's an example flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: Reader Adapter\n",
    "\n",
    "An instance of a reader adapter can be used as an entry point in the pipeline. It reads an input file `input.csv` file and a `input.yaml` file describing the D-REPR layout of this file. The data are representated in general way in a python object (Graph or NumPY array) and will be used in the next steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {input.csv, input.yaml} --> python_graph_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: Transformation Adapter\n",
    "\n",
    "An instance of a transformation adapter does not materialize the data into an output, it just reproduces the data, transformaing its content (the actual data) and performing the needed configrations in the 'control plane' parameters.\n",
    "Given the pythonic object (graph) representing the data and the rest of the needed configurations in the calling API ('control plane' parameters) we can re-construct the data in a new pythonic object (graph) and prepare it for the next steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_graph_obj --> python_graph_obj*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: Writer Adapter\n",
    "\n",
    "An instance of a writer adapter can be used as an exit point in the pipeline. It writes an output file `output.csv` based on a given `output.yaml` (D-REPR layout) and an additional configuration file `output.config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {python_graph_obj*, output.yaml, output.config} --> {output.csv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
